{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data \n",
    "link = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "path = tf.keras.utils.get_file('mnist.npz',link)\n",
    "data = np.load(path)\n",
    "x_coord_train , y_coord_train = data[\"x_train\"], data[\"y_train\"]\n",
    "x_coord_test , y_coord_test = data[\"x_test\"], data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = [0, 1] # making the array for the classes we want \n",
    "select_val_train = np.isin(y_coord_train, selected_classes) # using the np.isin function to get the selected classes\n",
    "select_val_test = np.isin(y_coord_test, selected_classes) # using the np.isin function to get the selected classes\n",
    "x_selected_train = x_coord_train[select_val_train] #selected x_train data -- the value of n is close to 18000\n",
    "y_selected_train = y_coord_train[select_val_train] #selected y_train data -- the value of n is close to 18000\n",
    "y_selected_train = np.where(y_selected_train == 0, -1, 1)  # changing the label 0 to -1 and 1 will remain 1\n",
    "x_selected_test = x_coord_test[select_val_test] #selected x_test data -- the value of n is close to 3000\n",
    "y_selected_test = y_coord_test[select_val_test] #selected y_test data -- the value of n is close to 3000\n",
    "y_selected_test = np.where(y_selected_test == 0, -1, 1) # changing the label 0 to -1 and 1 will remain 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the train set into train and val set. Keep 1000 samples from each\n",
    "class for val. Note val should be used to evaluate the performance of the\n",
    "classifier. Must not be used in obtaining PCA matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# dividing train set into train and val set . \n",
    "# indices_class_neg_1 = np.where(y_selected_train == -1)[0] # getting the indices of class -1\n",
    "# indices_class_1 = np.where(y_selected_train == 1)[0] # getting the indices of class 1\n",
    "# val_indices = np.concatenate((indices_class_neg_1[:1000] , indices_class_1[:1000])) # taking 1000 samples from each class \n",
    "# train_indices = np.concatenate((indices_class_neg_1[1000:] , indices_class_1[1000:])) # taking the rest of the samples for training\n",
    "val_points = [] ; val_y = [] ; train_points = [] ; y_train_points = []\n",
    "idx_class_neg1 = np.where(y_selected_train == -1)[0]\n",
    "idx_class_1 = np.where(y_selected_train == 1)[0]\n",
    "val_points.extend(x_selected_train[idx_class_neg1[:1000]])\n",
    "val_points.extend(x_selected_train[idx_class_1[:1000]])\n",
    "val_y.extend(y_selected_train[idx_class_neg1[:1000]])\n",
    "val_y.extend(y_selected_train[idx_class_1[:1000]])\n",
    "train_points.extend(x_selected_train[idx_class_neg1[1000:]])\n",
    "train_points.extend(x_selected_train[idx_class_1[1000:]])\n",
    "y_train_points.extend(y_selected_train[idx_class_neg1[1000:]])\n",
    "y_train_points.extend(y_selected_train[idx_class_1[1000:]])\n",
    "\n",
    "val_points = np.array(val_points) ; val_y = np.array(val_y) ; train_points = np.array(train_points) ; y_train_points = np.array(y_train_points)\n",
    "print(val_points.shape)\n",
    "# val_points = y_selected_train[val_indices] # getting the validation set\n",
    "# train_points = x_selected_train[train_indices] # getting the validation set\n",
    "# y_train_points = y_selected_train[train_indices] # getting the labels for the training set\n",
    "# print(val_points.shape)\n",
    "# print(train_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA and reduce the dimension to p = 5. You can use the train set\n",
    "of the two classes to obtain PCA matrix. For the remaining parts, use the\n",
    "reduced dimension dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10665)\n",
      "(5, 2000)\n"
     ]
    }
   ],
   "source": [
    "# applying PCA matrix using the train_points \n",
    "p = 5\n",
    "train_points = train_points.reshape(train_points.shape[0], -1)\n",
    "# print(train_points.shape)\n",
    "train_points = train_points.T \n",
    "mean_train_points = np.mean(train_points , axis=1 , keepdims=True)\n",
    "train_centered = train_points - mean_train_points\n",
    "S = (train_centered @ train_centered.T) / (train_centered.shape[1]- 1)\n",
    "S_eigenvalues, S_eigenvectors = np.linalg.eig(S)\n",
    "sorted_S = np.argsort(S_eigenvalues)[::-1]\n",
    "S_eigenvalues = S_eigenvalues[sorted_S]\n",
    "S_eigenvectors = S_eigenvectors[:, sorted_S]\n",
    "U = S_eigenvectors\n",
    "U_p = U[:, :p]\n",
    "Y = U_p.T @ train_centered # applying the PCA matrix on the train_points to reduce the dimension of the data\n",
    "\n",
    "print(Y.shape)\n",
    "\n",
    "val_points = val_points.reshape(val_points.shape[0], -1)\n",
    "val_points = val_points.T\n",
    "mean_val_points = np.mean(val_points , axis=1 , keepdims=True)\n",
    "val_centered = val_points - mean_val_points\n",
    "S_val = (val_centered @ val_centered.T) / (val_centered.shape[1]- 1)\n",
    "S_eigenvalues_val, S_eigenvectors_val = np.linalg.eig(S_val)\n",
    "sorted_S_val = np.argsort(S_eigenvalues_val)[::-1]\n",
    "S_eigenvalues_val = S_eigenvalues_val[sorted_S_val]\n",
    "S_eigenvectors_val = S_eigenvectors_val[:, sorted_S_val]\n",
    "U_val = S_eigenvectors_val\n",
    "U_p_val = U_val[:, :p]\n",
    "X_val = U_p_val.T @ val_centered # applying the PCA matrix on the val_points to reduce the dimension of the data\n",
    "print(X_val.shape)\n",
    "\n",
    "test_points = x_selected_test.reshape(x_selected_test.shape[0], -1)\n",
    "test_points = test_points.T\n",
    "#mean_test_points = np.mean(test_points , axis=1 , keepdims=True)\n",
    "test_centered = test_points - mean_train_points\n",
    "S_test = (test_centered @ test_centered.T) / (test_centered.shape[1]- 1)\n",
    "S_eigenvalues_test, S_eigenvectors_test = np.linalg.eig(S_test)\n",
    "sorted_S_test = np.argsort(S_eigenvalues_test)[::-1]\n",
    "S_eigenvalues_test = S_eigenvalues_test[sorted_S_test]\n",
    "S_eigenvectors_test = S_eigenvectors_test[:, sorted_S_test]\n",
    "U_test = S_eigenvectors\n",
    "U_p_test = U_test[:, :p]\n",
    "X_test = U_p_test.T @ test_centered # applying the PCA matrix on the test_points to reduce the dimension of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(weight, unique , train_points , y_train_points): \n",
    "    min_loss = 1.5  ; min_mid = 0 ; indexes_weight_changed = []  ; final_left_predict = 0 ; final_right_predict = 0 \n",
    "    mid_vals = ((unique[1:] + unique[:-1])/2)\n",
    "    for j in range(len(mid_vals)):\n",
    "        weights_changed_indices = []\n",
    "        # predicted_left = [] \n",
    "        # predicted_right = [] \n",
    "        mid = mid_vals[j] \n",
    "        # print(mid) \n",
    "        train_points = np.array(train_points) ; weight = np.array(weight)\n",
    "        # for k in range(len(train_points[dim])) : \n",
    "        #     if train_points[dim][k] < mid : \n",
    "        #         predicted_left.append(y_train_points[k])\n",
    "        #         predicted_left_indices.append(k)\n",
    "        #     else : \n",
    "        #         predicted_right.append(y_train_points[k])\n",
    "        #         predicted_right_indices.append(k)\n",
    "        # train_points = train_points.T     \n",
    "        # decision_split = train_points[:,dim] < mid\n",
    "        # # print(decision_split)\n",
    "        # decision_less = weight[decision_split]\n",
    "        # decision_more = weight[~decision_split]\n",
    "        # less_chosen = y_train_points[decision_split]\n",
    "        # more_chosen = y_train_points[~decision_split]\n",
    "        # indices_less = np.where(decision_split)[0]\n",
    "        # indices_more = np.where(~decision_split)[0]\n",
    "        # train_points = train_points.T\n",
    "        left_points = y_train_points[0:j] # taking first j points\n",
    "        right_points = y_train_points[j:len(y_train_points)]  # taking the rest of the points\n",
    "        weight_left = weight[0:j]   # taking the weights of the first j points\n",
    "        weight_right = weight[j:len(weight)] # taking the weights of the rest of the points\n",
    "        left_predict = 1 if np.sum(left_points) > 0 else -1 # making left prediction  \n",
    "        right_predict = 1 if np.sum(right_points) > 0 else -1 # making right prediction \n",
    "        # count_neg1_left = predicted_left.count(-1) ; count_1_left = predicted_left.count(1) ; count_neg1_right = predicted_right.count(-1) ; count_1_right = predicted_right.count(1)   \n",
    "        #print(count_neg1_left , count_1_left , count_neg1_right , count_1_right)\n",
    "        # if count_neg1_left > count_1_left:\n",
    "        #     left_predict = (-1) \n",
    "        # else: \n",
    "        #     left_predict = 1\n",
    "        # if count_neg1_right > count_1_right:\n",
    "        #     right_predict = (-1)\n",
    "        # else:\n",
    "        #     right_predict = 1    \n",
    "        # count_neg1_left = np.sum(less_chosen == -1) ; count_1_left = np.sum(less_chosen == 1) ; count_neg1_right = np.sum(more_chosen == -1) ; count_1_right = np.sum(more_chosen == 1)\n",
    "        # # print(count_neg1_left , count_1_left , count_neg1_right , count_1_right)\n",
    "        # left_predict = -1 if count_neg1_left > count_1_left else 1\n",
    "        # right_predict = -1 if count_neg1_right > count_1_right else 1   \n",
    "        # missclassified_weight = 0  ; misclassified_less = 0 ; misclassified_more = 0    \n",
    "        # misclassified_less = less_chosen != left_predict\n",
    "        # misclassified_more = more_chosen != right_predict\n",
    "        # missclassified_weight += np.sum(decision_less[misclassified_less])\n",
    "        # missclassified_weight += np.sum(decision_more[misclassified_more])\n",
    "        # weights_changed_indices.extend(indices_less[misclassified_less])\n",
    "        # weights_changed_indices.extend(indices_more[misclassified_more])\n",
    "        # for k in range(len(predicted_left)):\n",
    "        #     total_weight += weight[predicted_left_indices[k]]            \n",
    "        #     if predicted_left[k] != left_predict:\n",
    "        #         missclassified_weight += weight[predicted_left_indices[k]]\n",
    "        #         weights_changed_indices.append(predicted_left_indices[k])\n",
    "        #         # print(\"1\")\n",
    "        # for k in range(len(predicted_right)):\n",
    "        #     total_weight += weight[predicted_right_indices[k]]\n",
    "        #     if predicted_right[k] != right_predict:\n",
    "        #         missclassified_weight += weight[predicted_right_indices[k]]\n",
    "        #         weights_changed_indices.append(predicted_right_indices[k])         \n",
    "                # print(\"0\")\n",
    "        # print(total_weight)        \n",
    "        # print(missclassified_weight / total_weight )        \n",
    "        missclassified_weight = np.sum(weight_left[left_points != left_predict]) + np.sum(weight_right[right_points != right_predict]) ; total_weight = np.sum(weight)\n",
    "        if (missclassified_weight / total_weight) < min_loss:\n",
    "            min_loss = (missclassified_weight / total_weight)\n",
    "            min_mid = mid\n",
    "            indexes_weight_changed = weights_changed_indices\n",
    "            final_left_predict = left_predict\n",
    "            final_right_predict = right_predict\n",
    "            \n",
    "    return min_loss , min_mid , indexes_weight_changed , final_left_predict , final_right_predict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now learn a decision tree using the train set. You need to grow a deci-\n",
    "sion stump. For each dimension, find the unique values and sort them\n",
    "\n",
    "in ascending order. The splits to be evaluated will be midpoint of two\n",
    "consecutive unique values. Find the best split by minimizing weighted\n",
    "\n",
    "1\n",
    "\n",
    "miss-classification error. Denote this as h1(x). Note as we are dealing\n",
    "with real numbers, each value may be unique. So just sorting them and\n",
    "taking midpoint of consecutive values may also result in similar tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum loss is coming at dimension: 0 and loss is coming : 0.004688232536333803 and the split is happening at : (192.72278407732898+0j)\n",
      "Alpha is :  5.35800036870921\n",
      "(5, 10665)\n",
      "[9.37646507e-05 9.37646507e-05 9.37646507e-05 ... 9.37646507e-05\n",
      " 9.37646507e-05 9.37646507e-05]\n",
      "The accuracy for 1 trees is : 99.5 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy is coming at 1 trees and the accuracy is 99.5 %\n",
      "The accuracy for the test set is : 99.62174940898345 %\n"
     ]
    }
   ],
   "source": [
    "def learn_tree(train_set , weights):\n",
    "    # train_set[i][j] -- means that we are at ith dimension and at jth point\n",
    "    indexes_changed = [] ; midpoints=[] ; left_predictions = [] ; right_predictions = []\n",
    "    # print(weights)\n",
    "    min_losses = []\n",
    "    for i in range(5):\n",
    "        unique_points = np.unique(train_set[i]) \n",
    "        # for j in range(len(train_set[i])):\n",
    "        #     if train_set[i][j] not in unique_points:\n",
    "        #         unique_points.append(train_set[i][j])\n",
    "        # unique_points.sort()\n",
    "        # unique_points = np.random.choice(unique_points , 1000,replace=False)\n",
    "        # print(len(unique_points))\n",
    "        # print(unique_points)\n",
    "        # print(weights)\n",
    "        sorted_idx = np.argsort(train_set[i])\n",
    "        train_set_sorted = train_set.T[sorted_idx].T\n",
    "        y_train_points_sorted = y_train_points[sorted_idx] ; weights_sorted = np.array(weights)[sorted_idx]\n",
    "        # print(train_set_sorted.shape , y_train_points_sorted.shape)\n",
    "        min_loss , min_mid  , indexes_c , left_predict , right_predict= loss(weights_sorted , unique_points , train_set_sorted , y_train_points_sorted)\n",
    "        midpoints.append(min_mid)\n",
    "        # print(len(indexes_c))\n",
    "        indexes_changed.append(indexes_c)\n",
    "        \n",
    "        # print(min_loss , min_mid)\n",
    "        min_losses.append(min_loss)\n",
    "        left_predictions.append(left_predict)                                                        \n",
    "        right_predictions.append(right_predict)\n",
    "        # print(unique_points)     \n",
    "    min_dim = 0 ; min_loss_val = min_losses[0]  ; minloss_indexes = indexes_changed[0] ; splitat = midpoints[0] ; final_left_predict = left_predictions[0] ; final_right_predict = right_predictions[0]\n",
    "    for i in range(1,len(min_losses)):\n",
    "        if min_losses[i] < min_loss_val:\n",
    "            min_loss_val = min_losses[i]\n",
    "            min_dim = i\n",
    "            minloss_indexes = indexes_changed[i]\n",
    "            splitat = midpoints[i]\n",
    "            final_left_predict = left_predictions[i]    \n",
    "            final_right_predict = right_predictions[i]\n",
    "    print(\"Minimum loss is coming at dimension:\",min_dim , \"and loss is coming :\",min_loss_val , \"and the split is happening at :\",splitat)\n",
    "    \n",
    "# print(Y.shape)\n",
    "    alpha1 = np.log((1 - min_loss_val) / min_loss_val)\n",
    "    print(\"Alpha is : \",alpha1)\n",
    "    # print(minloss_indexes)\n",
    "    sorted_idx = np.argsort(train_set[min_dim])\n",
    "    \n",
    "    train_set_sorted = train_set.T[sorted_idx].T\n",
    "    print(train_set_sorted.shape)\n",
    "    y_train_points_sorted = y_train_points[sorted_idx] \n",
    "    \n",
    "    sorted_weights = np.array(weights)[sorted_idx]\n",
    "    \n",
    "    y_sorted_left = y_train_points_sorted[train_set_sorted[min_dim] < splitat] \n",
    "    \n",
    "    y_sorted_right = y_train_points_sorted[train_set_sorted[min_dim] >= splitat]\n",
    "    \n",
    "    weights_less = sorted_weights[train_set_sorted[min_dim] < splitat]  \n",
    "    \n",
    "    weights_more = sorted_weights[train_set_sorted[min_dim] >= splitat]\n",
    "    \n",
    "    split_dec_left = 1 if np.sum(y_sorted_left) > 0 else -1 ; split_dec_right = 1 if np.sum(y_sorted_right) > 0 else -1\n",
    "    \n",
    "    weights_less[y_sorted_left != split_dec_left] = weights_less[y_sorted_left != split_dec_left] * ((1 - min_loss_val) / min_loss_val)\n",
    "    \n",
    "    weights_more[y_sorted_right != split_dec_right] = weights_more[y_sorted_right != split_dec_right] * ((1 - min_loss_val) / min_loss_val)\n",
    "    \n",
    "    sorted_weights = np.concatenate((weights_less , weights_more))\n",
    "    \n",
    "    first_idxeing = np.argsort(sorted_idx)\n",
    "    \n",
    "    weights = sorted_weights[first_idxeing]\n",
    "\n",
    "    \n",
    "        \n",
    "    # for i in range(len(minloss_indexes)):\n",
    "    #     weights[minloss_indexes[i]] = weights[minloss_indexes[i]] * ((1- min_loss_val) / min_loss_val)\n",
    "    # # print(weights[11])     \n",
    "    return min_dim , splitat , final_left_predict , final_right_predict , min_loss_val , alpha1 , weights\n",
    "weights = [1/(len(Y[0]))] * (len(Y[0])) ; stored_dimensions = [] ; stored_splits = [] ; stored_left_predict = [] ; stored_right_predict = [] ; accuracy = [] ; alphas =[]\n",
    "# making 300 decision stump trees\n",
    "for i in range(1):\n",
    "    min_dim , splitat , left_predict , right_predict , min_loss_val ,alpha1 , weights= learn_tree(Y , weights) \n",
    "    print(weights)\n",
    "    # print(weights[11])\n",
    "    stored_dimensions.append(min_dim) ; stored_splits.append(splitat) ; stored_left_predict.append(left_predict) ; stored_right_predict.append(right_predict) ; alphas.append(alpha1)\n",
    "    # print(stored_dimensions , stored_splits , stored_left_predict , stored_right_predict , alphas)\n",
    "    true_classified = 0 \n",
    "    for j in range(len(X_val[0])):\n",
    "        f_x = 0 \n",
    "        for k in range(0,i+1):\n",
    "            f_x += (alphas[k] * (stored_left_predict[k] if X_val[stored_dimensions[k]][j] < stored_splits[k] else stored_right_predict[k]))\n",
    "        if np.sign(f_x) == val_y[j]:\n",
    "            true_classified += 1\n",
    "    print(f\"The accuracy for {i+1} trees is : {true_classified / len(X_val[0]) * 100} %\")          \n",
    "    accuracy.append(true_classified / len(X_val[0]))\n",
    "plt.plot(range(1, 2), accuracy, marker='o', linestyle='-')\n",
    "plt.title('Accuracy vs. Number of Trees')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "no_of_trees = 1 ; max_accuracy = accuracy[0]\n",
    "for i in range(1 ,len(accuracy) ,1):\n",
    "    if accuracy[i] >= max_accuracy:\n",
    "        no_of_trees = i+1\n",
    "        max_accuracy = accuracy[i]\n",
    "print(f\"The maximum accuracy is coming at {no_of_trees} trees and the accuracy is {max_accuracy * 100} %\")\n",
    "\n",
    "# seeing accuracy for test set \n",
    "true_classified = 0\n",
    "for j in range(len(X_test[0])):\n",
    "    f_x = 0\n",
    "    for k in range(no_of_trees):\n",
    "        f_x += (alphas[k] * (stored_left_predict[k] if X_test[stored_dimensions[k]][j] < stored_splits[k] else stored_right_predict[k]))\n",
    "    if np.sign(f_x) == y_selected_test[j]:\n",
    "        true_classified += 1\n",
    "print(f\"The accuracy for the test set is : {true_classified / len(X_test[0]) * 100} %\")    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 1 trees is : 0.980483181186607\n"
     ]
    }
   ],
   "source": [
    "#Question - 2\n",
    "def loss_regressor(unique , train_points , y_train_points): \n",
    "    mid_vals = ((unique[1:] + unique[:-1])/2) ; SSR_arr = []; left_means = [] ;right_means = [] \n",
    "    for j in range(1,len(mid_vals)):\n",
    "        mid = mid_vals[j] \n",
    "        train_points = np.array(train_points) ; \n",
    "        left_points = y_train_points[0:j] # taking first j points\n",
    "        right_points = y_train_points[j:len(y_train_points)]  # taking the rest of the points\n",
    "        # print(left_points , right_points)\n",
    "        left_mean = np.mean(left_points) ; right_mean = np.mean(right_points)\n",
    "        left_points = left_points - left_mean ; left_points = left_points**2  ; right_points = right_points - right_mean ; right_points = right_points**2\n",
    "        sum_left = np.sum(left_points) ; sum_right = np.sum(right_points)\n",
    "        SSR = sum_left + sum_right\n",
    "        SSR_arr.append(SSR) ; \n",
    "        left_means.append(left_mean) ; right_means.append(right_mean)\n",
    "    min_loss = np.min(SSR_arr) ; min_mid = mid_vals[np.argmin(SSR_arr)] ; min_left_mean = left_means[np.argmin(SSR_arr)] ; min_right_mean = right_means[np.argmin(SSR_arr)]\n",
    "    return min_loss , min_mid , min_left_mean , min_right_mean \n",
    "        \n",
    "def learn_tree_regressor(train_set , y_train_points):\n",
    "    losses = [] ; midpoints = [] ; left_means = [] ; right_means = []\n",
    "    for i in range(5):\n",
    "        unique_points = np.unique(train_set[i]) \n",
    "        sorted_idx = np.argsort(train_set[i])\n",
    "        train_set_sorted = train_set.T[sorted_idx].T\n",
    "        y_train_points_sorted = y_train_points[sorted_idx]\n",
    "        min_loss_i , min_mid_i , min_left_mean_i , min_right_mean_i = loss_regressor(unique_points , train_set_sorted , y_train_points_sorted)\n",
    "        losses.append(min_loss_i) ; midpoints.append(min_mid_i) ; left_means.append(min_left_mean_i) ; right_means.append(min_right_mean_i)\n",
    "    min_dim = 0 ; min_loss_val = losses[0] ; splitat = midpoints[0] ; final_left_mean = left_means[0] ; final_right_mean = right_means[0]\n",
    "    for i in range(1,len(losses)):\n",
    "        if losses[i] < min_loss_val:\n",
    "            min_loss_val = losses[i]\n",
    "            min_dim = i\n",
    "            splitat = midpoints[i]\n",
    "            final_left_mean = left_means[i]    \n",
    "            final_right_mean = right_means[i]\n",
    "    sorted_idx = np.argsort(train_set[min_dim])\n",
    "    train_set_sorted = train_set.T[sorted_idx].T\n",
    "    print(train_set_sorted.shape )\n",
    "    y_train_points_sorted = y_train_points[sorted_idx]\n",
    "    sorted_y_train_less = y_train_points_sorted[train_set_sorted[min_dim] < splitat]\n",
    "    sorted_y_train_more = y_train_points_sorted[train_set_sorted[min_dim] >= splitat]\n",
    "    sorted_y_train_less = np.where(sorted_y_train_less - 0.01*final_left_mean > 0 , 1 , -1)\n",
    "    sorted_y_train_more = np.where(sorted_y_train_more - 0.01*final_right_mean > 0 , 1 , -1)\n",
    "    sorted_y_train_points = np.concatenate((sorted_y_train_less , sorted_y_train_more))\n",
    "    first_idxeing = np.argsort(sorted_idx)\n",
    "    y_train_points = sorted_y_train_points[first_idxeing]\n",
    "            \n",
    "    print(f\"The minimum loss is coming as: {min_loss_val} at dimension: {min_dim} and the split is happening at: {splitat}\")\n",
    "    return min_dim , splitat , final_left_mean , final_right_mean , min_loss_val , y_train_points  \n",
    "dimensions = [] ; splits = [] ; left_means = [] ; right_means = []  ; MSE_arr = []\n",
    "for i in range(1):\n",
    "    min_dim , splitat , left_mean , right_mean , min_loss_val , y_train_points = learn_tree_regressor(Y , y_train_points)\n",
    "    dimensions.append(min_dim) ; splits.append(splitat) ; left_means.append(left_mean) ; right_means.append(right_mean)\n",
    "    MSE = 0 \n",
    "    for j in range(len(X_val[0])):\n",
    "        f_x = 0\n",
    "        for k in range(i+1):\n",
    "            f_x += 0.01*( left_means[k]   if X_val[dimensions[k]][j] < splits[k] else right_means[k]) \n",
    "        MSE += (f_x - val_y[j])**2    \n",
    "    MSE = MSE / len(X_val[0])\n",
    "    MSE_arr.append(MSE) \n",
    "    print(f\"The MSE for {i+1} trees is : {MSE}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, 2), MSE_arr)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE vs. Number of Trees')\n",
    "plt.grid(True)\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum MSE is coming at 1 trees and the minimum MSE is 0.980483181186607\n",
      "The MSE for the test set is : 0.980435512804344\n"
     ]
    }
   ],
   "source": [
    "no_of_trees = 1 ; min_MSE = MSE_arr[0]\n",
    "for i in range(1 ,len(MSE_arr) ,1):\n",
    "    if MSE_arr[i] <= min_MSE:\n",
    "        no_of_trees = i+1\n",
    "        min_MSE = MSE_arr[i]\n",
    "print(f\"The minimum MSE is coming at {no_of_trees} trees and the minimum MSE is {min_MSE}\")\n",
    "\n",
    "MSE_test = 0 \n",
    "for j in range(len(X_test[0])):\n",
    "    f_x = 0\n",
    "    for k in range(no_of_trees):\n",
    "        f_x += 0.01*( left_means[k]   if X_test[dimensions[k]][j] < splits[k] else right_means[k])    \n",
    "    MSE_test += (f_x - y_selected_test[j])**2\n",
    "print(f\"The MSE for the test set is : {MSE_test / len(X_test[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 1 trees is : 0.980483181186607\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 2 trees is : 0.9611626416596478\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 3 trees is : 0.9420383814188985\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 4 trees is : 0.9231104004644752\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 5 trees is : 0.9043786987963917\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 6 trees is : 0.8858432764146039\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 7 trees is : 0.8675041333190918\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 8 trees is : 0.8493612695099724\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 9 trees is : 0.8314146849871772\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 10 trees is : 0.8136643797506103\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 11 trees is : 0.7961103538004057\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 12 trees is : 0.7787526071365715\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 13 trees is : 0.7615911397589763\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 14 trees is : 0.7446259516677269\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 15 trees is : 0.7278570428627872\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 16 trees is : 0.7112844133441211\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 17 trees is : 0.6949080631118323\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 18 trees is : 0.6787279921658415\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 19 trees is : 0.6627442005061428\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 20 trees is : 0.646956688132774\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 21 trees is : 0.6313654550457392\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 22 trees is : 0.615970501245024\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 23 trees is : 0.6007718267306064\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 24 trees is : 0.5857694315025124\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 25 trees is : 0.5709633155606938\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 26 trees is : 0.5563534789052376\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 27 trees is : 0.5419399215360755\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 28 trees is : 0.5277226434532384\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 29 trees is : 0.5137016446567177\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 30 trees is : 0.4998769251465372\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 31 trees is : 0.48624848492261985\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 32 trees is : 0.47281632398503726\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 33 trees is : 0.45958044233377593\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 34 trees is : 0.4465408399688315\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 35 trees is : 0.4336975168902234\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 36 trees is : 0.4210504730978987\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 37 trees is : 0.4085997085918909\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 38 trees is : 0.3963452233722061\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 39 trees is : 0.38428701743885013\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 40 trees is : 0.372425090791785\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 41 trees is : 0.360759443431065\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 42 trees is : 0.34929007535664874\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 43 trees is : 0.33801698656853635\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 44 trees is : 0.32694017706673306\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 45 trees is : 0.3160596468512752\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 46 trees is : 0.30537539592210217\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 47 trees is : 0.29488742427925396\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 48 trees is : 0.28459573192273746\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 49 trees is : 0.2745003188525198\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 50 trees is : 0.2646011850686275\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 51 trees is : 0.25489833057103306\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 52 trees is : 0.24539175535976623\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 53 trees is : 0.2360814594348318\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 54 trees is : 0.22696744279619233\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 55 trees is : 0.21804970544387858\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 56 trees is : 0.2093282473778672\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 57 trees is : 0.2008030685981819\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 58 trees is : 0.19247416910480059\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 59 trees is : 0.18434154889776028\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 60 trees is : 0.17640520797700748\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 61 trees is : 0.16866514634258417\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 62 trees is : 0.1611213639944737\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 63 trees is : 0.15377386093267728\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 64 trees is : 0.1466226371572027\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 65 trees is : 0.13966769266804108\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 66 trees is : 0.13290902746519168\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 67 trees is : 0.12634664154865377\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 68 trees is : 0.11998053491844586\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 69 trees is : 0.11381070757453512\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 70 trees is : 0.1078371595169515\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 71 trees is : 0.10205989074568166\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 72 trees is : 0.09647890126072696\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 73 trees is : 0.09109419106209277\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 74 trees is : 0.08590576014976525\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 75 trees is : 0.08091360852375926\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 76 trees is : 0.07611773618406491\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 77 trees is : 0.07151814313068418\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 78 trees is : 0.06711482936362792\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 79 trees is : 0.06290779488287913\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 80 trees is : 0.058897039688450926\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 81 trees is : 0.055082563780335506\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 82 trees is : 0.05146436715853324\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 83 trees is : 0.048042449823051885\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 84 trees is : 0.0448168117738834\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 85 trees is : 0.04178745301102803\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 86 trees is : 0.03895437353449094\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 87 trees is : 0.036317573344269605\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 88 trees is : 0.0338770524403641\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 89 trees is : 0.031632810822773\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 90 trees is : 0.029584848491498315\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 91 trees is : 0.027733165446537502\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 92 trees is : 0.02607776168789372\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 93 trees is : 0.024618637215565446\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 94 trees is : 0.023355792029552326\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 95 trees is : 0.0222892261298551\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 96 trees is : 0.021418939516472453\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 97 trees is : 0.020744932189406925\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 98 trees is : 0.020267204148655404\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 99 trees is : 0.01998575539422013\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 100 trees is : 0.019900585926100105\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 101 trees is : 0.020011695744295507\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 0.0 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 102 trees is : 0.019900475500779613\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 103 trees is : 0.02000973371179116\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 0.0 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 104 trees is : 0.01990038257477428\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 199.06068291042573 at dimension: 0 and the split is happening at: (189.85586681021414+0j)\n",
      "The MSE for 105 trees is : 0.020007789178601302\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 110.98316616766233 at dimension: 3 and the split is happening at: (168.89744536772724+0j)\n",
      "The MSE for 106 trees is : 0.020097796854538635\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 87.59645026706691 at dimension: 3 and the split is happening at: (84.9624451819993+0j)\n",
      "The MSE for 107 trees is : 0.020007415451424565\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 2450.090827123433 at dimension: 3 and the split is happening at: (85.00169485495394+0j)\n",
      "The MSE for 108 trees is : 0.020059354690665752\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 87.59645026706691 at dimension: 3 and the split is happening at: (84.9624451819993+0j)\n",
      "The MSE for 109 trees is : 0.019996880440471307\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 110.98316616766233 at dimension: 3 and the split is happening at: (168.89744536772724+0j)\n",
      "The MSE for 110 trees is : 0.02005884276040257\n",
      "(5, 10665)\n",
      "The minimum loss is coming as: 87.59645026706691 at dimension: 3 and the split is happening at: (84.9624451819993+0j)\n",
      "The MSE for 111 trees is : 0.01999666709426194\n"
     ]
    }
   ],
   "source": [
    "#Question - 2\n",
    "def loss_regressor(unique , train_points , y_train_points): \n",
    "    mid_vals = ((unique[1:] + unique[:-1])/2) ; SSR_arr = []; left_means = [] ;right_means = [] \n",
    "    for j in range(1,len(mid_vals)):\n",
    "        mid = mid_vals[j] \n",
    "        train_points = np.array(train_points) ; \n",
    "        left_points = y_train_points[0:j] # taking first j points\n",
    "        right_points = y_train_points[j:len(y_train_points)]  # taking the rest of the points\n",
    "        # print(left_points , right_points)\n",
    "        left_mean = np.mean(left_points) ; right_mean = np.mean(right_points)\n",
    "        left_points = left_points - left_mean ; left_points = left_points**2  ; right_points = right_points - right_mean ; right_points = right_points**2\n",
    "        sum_left = np.sum(left_points) ; sum_right = np.sum(right_points)\n",
    "        SSR = sum_left + sum_right\n",
    "        SSR_arr.append(SSR) ; \n",
    "        left_means.append(left_mean) ; right_means.append(right_mean)\n",
    "    min_loss = np.min(SSR_arr) ; min_mid = mid_vals[np.argmin(SSR_arr)] ; min_left_mean = left_means[np.argmin(SSR_arr)] ; min_right_mean = right_means[np.argmin(SSR_arr)]\n",
    "    return min_loss , min_mid , min_left_mean , min_right_mean \n",
    "        \n",
    "def learn_tree_regressor(train_set , y_train_points , residue):\n",
    "    losses = [] ; midpoints = [] ; left_means = [] ; right_means = []\n",
    "    for i in range(5):\n",
    "        unique_points = np.unique(train_set[i]) \n",
    "        sorted_idx = np.argsort(train_set[i])\n",
    "        train_set_sorted = train_set.T[sorted_idx].T\n",
    "        y_train_points_sorted = y_train_points[sorted_idx]\n",
    "        min_loss_i , min_mid_i , min_left_mean_i , min_right_mean_i = loss_regressor(unique_points , train_set_sorted , y_train_points_sorted)\n",
    "        losses.append(min_loss_i) ; midpoints.append(min_mid_i) ; left_means.append(min_left_mean_i) ; right_means.append(min_right_mean_i)\n",
    "    min_dim = 0 ; min_loss_val = losses[0] ; splitat = midpoints[0] ; final_left_mean = left_means[0] ; final_right_mean = right_means[0]\n",
    "    for i in range(1,len(losses)):\n",
    "        if losses[i] < min_loss_val:\n",
    "            min_loss_val = losses[i]\n",
    "            min_dim = i\n",
    "            splitat = midpoints[i]\n",
    "            final_left_mean = left_means[i]    \n",
    "            final_right_mean = right_means[i]\n",
    "    sorted_idx = np.argsort(train_set[min_dim])\n",
    "    train_set_sorted = train_set.T[sorted_idx].T\n",
    "    print(train_set_sorted.shape )\n",
    "    y_train_points_sorted = y_train_points[sorted_idx]\n",
    "    residue_sorted = residue[sorted_idx]\n",
    "    residue_less = residue_sorted[train_set_sorted[min_dim] < splitat] - 0.01*final_left_mean\n",
    "    residue_more = residue_sorted[train_set_sorted[min_dim] >= splitat] - 0.01*final_right_mean\n",
    "    sorted_y_train_less = y_train_points_sorted[train_set_sorted[min_dim] < splitat]\n",
    "    sorted_y_train_more = y_train_points_sorted[train_set_sorted[min_dim] >= splitat]\n",
    "    sorted_y_train_less = np.where(residue_less >= 0 , 1 , -1)\n",
    "    sorted_y_train_more = np.where(residue_more >= 0 , 1 , -1)\n",
    "    sorted_y_train_points = np.concatenate((sorted_y_train_less , sorted_y_train_more))\n",
    "    first_idxeing = np.argsort(sorted_idx)\n",
    "    y_train_points = sorted_y_train_points[first_idxeing]\n",
    "    residue_new = np.concatenate((residue_less , residue_more))\n",
    "    residue = residue_new[first_idxeing]           \n",
    "    print(f\"The minimum loss is coming as: {min_loss_val} at dimension: {min_dim} and the split is happening at: {splitat}\")\n",
    "    return min_dim , splitat , final_left_mean , final_right_mean , min_loss_val , y_train_points , residue \n",
    "dimensions = [] ; splits = [] ; left_means = [] ; right_means = []  ; MSE_arr = [] ; residue = y_train_points\n",
    "for i in range(300):\n",
    "    min_dim , splitat , left_mean , right_mean , min_loss_val , y_train_points , residue= learn_tree_regressor(Y , y_train_points ,residue)\n",
    "    dimensions.append(min_dim) ; splits.append(splitat) ; left_means.append(left_mean) ; right_means.append(right_mean)\n",
    "    MSE = 0 \n",
    "    for j in range(len(X_val[0])):\n",
    "        f_x = 0\n",
    "        for k in range(i+1):\n",
    "            f_x += 0.01*( left_means[k]   if X_val[dimensions[k]][j] < splits[k] else right_means[k]) \n",
    "        MSE += (f_x - val_y[j])**2    \n",
    "    MSE = MSE / len(X_val[0])\n",
    "    MSE_arr.append(MSE) \n",
    "    print(f\"The MSE for {i+1} trees is : {MSE}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 301), MSE_arr)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE vs. Number of Trees')\n",
    "plt.grid(True)\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_trees = 1 ; min_MSE = MSE_arr[0]\n",
    "for i in range(1 ,len(MSE_arr) ,1):\n",
    "    if MSE_arr[i] <= min_MSE:\n",
    "        no_of_trees = i+1\n",
    "        min_MSE = MSE_arr[i]\n",
    "print(f\"The minimum MSE is coming at {no_of_trees} trees and the minimum MSE is {min_MSE}\")\n",
    "\n",
    "MSE_test = 0 \n",
    "for j in range(len(X_test[0])):\n",
    "    f_x = 0\n",
    "    for k in range(no_of_trees):\n",
    "        f_x += 0.01*( left_means[k]   if X_test[dimensions[k]][j] < splits[k] else right_means[k])    \n",
    "    MSE_test += (f_x - y_selected_test[j])**2\n",
    "print(f\"The MSE for the test set is : {MSE_test / len(X_test[0])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
